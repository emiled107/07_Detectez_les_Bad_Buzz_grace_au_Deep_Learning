{"cells":[{"cell_type":"markdown","metadata":{"id":"2XD5y0byWcNh"},"source":["# Projet Air Paradis - Anticipation des Bad Buzz sur les Réseaux Sociaux\n","\n","## Contexte\n","Nous sommes engagés par Air Paradis, une compagnie aérienne qui souhaite développer un produit IA capable de prédire le sentiment associé à un tweet pour anticiper les bad buzz sur les réseaux sociaux. Ce notebook documente notre démarche de développement, de la conception des modèles jusqu'à leur déploiement.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5336,"status":"ok","timestamp":1715370627018,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"GJNs9r87DSqV","outputId":"e48954ef-61d5-4a89-97a3-26275b0ddfb3"},"outputs":[],"source":["import os\n","# Configurer l'environnement pour utiliser `tf_keras`\n","os.environ['TF_USE_LEGACY_KERAS'] = 'True'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6205,"status":"ok","timestamp":1715370633220,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"72auk9hQWcNu"},"outputs":[],"source":["# Standard libraries\n","import os\n","import re\n","from io import BytesIO\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Data manipulation and analysis\n","import pandas as pd\n","import numpy as np\n","\n","# Machine learning and deep learning libraries\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, auc\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import  ModelCheckpoint\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import GlobalMaxPooling1D\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, BatchNormalization\n","from tensorflow.keras.initializers import GlorotUniform\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.metrics import Precision, Recall\n","\n","from transformers import BertTokenizer, BertModel, BertConfig, TFBertForSequenceClassification\n","\n","import mlflow\n","import mlflow.sklearn\n","\n","# Natural Language Processing (NLP)\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","\n","import fasttext\n","from langdetect import detect, DetectorFactory, LangDetectException\n","\n","# Visualization libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","\n","# File and web handling\n","import requests\n","import zipfile\n","from urllib.request import urlretrieve\n","from zipfile import ZipFile\n","\n","# Utilities\n","import ftfy\n","import ast\n","from tqdm import tqdm\n","\n","# Machine learning models from gensim\n","from gensim.models import KeyedVectors\n","\n","import pickle\n","\n","import joblib  # Pour la sauvegarde du modèle\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Simple test operation\n","tf.constant([1.0, 2.0, 3.0]) + tf.constant([1.0, 2.0, 3.0])\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{"id":"43KtaHBEo4Qu"},"source":["#### Configuration initiale pour MLFLOW"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configuration du tracking URI pour MLflow\n","mlflow.set_tracking_uri(\"http://127.0.0.1:5003\")"]},{"cell_type":"markdown","metadata":{"id":"LfnzQV2pJL9u"},"source":["### Arret du notebook si le fichier traité existe deja pour passer a la suite"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":9368,"status":"ok","timestamp":1715370642586,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"s8X9Cn4_JLbB","outputId":"1fb74d86-ea2d-4151-9443-b082bd465c1d"},"outputs":[],"source":["# Chemin du fichier\n","file_path = '../data/data_english_only.csv'\n","\n","# Vérifier si le fichier existe\n","if os.path.exists(file_path):\n","    # Charger le DataFrame si le fichier existe\n","    data_english_only = pd.read_csv(file_path)\n","    display(data_english_only.head())  # Afficher les premières lignes pour vérifier le chargement\n","    # Stopper l'exécution du notebook\n","    print(\"Le fichier existe déjà. Chargement effectué. Arrêt du notebook.\")\n","else:\n","    print(\"Le fichier n'existe pas. Continuez l'exécution du notebook.\")"]},{"cell_type":"markdown","metadata":{"id":"_Rriky4oWcNl"},"source":["## Collecte et Exploration des Données\n","\n","Nous utiliserons des données Open Source pour entraîner nos modèles. Cette section se concentre sur l'acquisition, l'exploration et la préparation initiale des données pour nos modèles de sentiment analysis."]},{"cell_type":"markdown","metadata":{"id":"vO-GT9uTWcNl"},"source":["About Columns:¶\n","\n","    target: the polarity of the tweet (0 = negative, 4 = positive)\n","\n","    ids: The id of the tweet ( 2087)\n","\n","    date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n","\n","    flag: The query (lyx) Column represents that If there is no query, then this value is NO_QUERY.\n","\n","    user: The user that tweeted (robotickilldozr)\n","\n","    text: The text of the tweet (Lyx is cool) ## Acknowledgements:\n","\n","    The official link regarding the dataset with resources about how it was generated is here The official paper detailing the approach is here\n","\n","    Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12. ## Aims and Objectives:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":827},"executionInfo":{"elapsed":72536,"status":"ok","timestamp":1715362633653,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"nn6Y7rY5WcNm","outputId":"3fe3a57a-1f03-42a5-b903-1908abacaf22"},"outputs":[],"source":["# Paramètres configurables\n","data_directory = \"../data/sources\"  # Répertoire pour stocker les données\n","zip_file_url = \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\"  # URL du fichier ZIP à télécharger\n","zip_file_name = \"sentiment140.zip\"  # Nom du fichier ZIP à télécharger\n","\n","# Vérifiez si le répertoire et les données existent déjà pour éviter le téléchargement et l'extraction répétés\n","if not os.path.exists(os.path.join(data_directory, zip_file_name.replace('.zip', ''))):\n","    # Création du répertoire s'il n'existe pas\n","    if not os.path.exists(data_directory):\n","        os.makedirs(data_directory)\n","\n","    # Téléchargement du fichier ZIP\n","    response = requests.get(zip_file_url)\n","    zip_content = BytesIO(response.content)\n","\n","    # Extraction du contenu du fichier ZIP\n","    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n","        zip_ref.extractall(data_directory)\n","else:\n","    print(\"Les données sont déjà téléchargées et extraites.\")\n","\n","# Recherche du fichier CSV dans le répertoire extrait\n","data_path = None\n","for root, dirs, files in os.walk(data_directory):\n","    for file in files:\n","        if file.endswith('.csv'):\n","            data_path = os.path.join(root, file)\n","            break\n","\n","if data_path:\n","    # Chargement et affichage des premières lignes du fichier CSV pour vérifier\n","    data = pd.read_csv(data_path, encoding=\"latin_1\",  names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n","    # Supposons que la colonne contenant le texte est la 5ème colonne\n","    data['text'] = data['text'].apply(ftfy.fix_text)\n","\n","    # Affichage des informations sur les données\n","    display(data.info())\n","    display(data.describe(include='all'))\n","    display(data.head())\n","else:\n","    print(\"Aucun fichier CSV trouvé dans le répertoire extrait.\")"]},{"cell_type":"markdown","metadata":{"id":"F8S4RPU3WcNm"},"source":["## Prétraitement des Données\n","\n","Avant de former nos modèles, il est crucial de nettoyer et de prétraiter nos données. Cette étape inclut la tokenisation, la lemmatisation, la suppression des stop-words, etc.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3564,"status":"ok","timestamp":1715362637209,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"QklN56qtWcNm","outputId":"5b1b44cd-874a-41a4-e780-da0c124e599e"},"outputs":[],"source":["# Analyse des doublons\n","print(\"Nombre de doublons avant suppression :\", data.duplicated(subset=['date', 'user','text']).sum())\n","doublons = data[data.duplicated()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1926,"status":"ok","timestamp":1715362639134,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"VevIV60PWcNn"},"outputs":[],"source":["# supression des colonnes inutiles\n","data = data.drop(['ids','flag'], axis=1)\n","# Suppression des doublons\n","data_clean = data.drop_duplicates(subset=['date', 'user', 'text']).copy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715362639134,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"giAvgL43WcNn","outputId":"1e673eec-3ccf-43ea-b994-4f85301166be"},"outputs":[],"source":["# Re-codage de la colonne 'target' de 0 et 4 à 0 et 1\n","data_clean['target'] = data_clean['target'].map({0: 0, 4: 1})\n","\n","# Vérification des premières lignes pour s'assurer que les modifications sont correctes\n","display(data_clean.head())"]},{"cell_type":"markdown","metadata":{"id":"XYpbmSH3WcNn"},"source":["## Distribution des Sentiments\n","Un graphique à barres pour visualiser la distribution des sentiments dans le dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"elapsed":3152,"status":"ok","timestamp":1715362642284,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"WQFmtm48WcNn","outputId":"545a3a7c-eda4-4644-c9f5-61a25b554770"},"outputs":[],"source":["plt.figure(figsize=(8, 6))\n","sns.countplot(x='target', data=data_clean)\n","plt.title('Distribution des Sentiments')\n","plt.xlabel('Sentiment')\n","plt.ylabel('Nombre de Tweets')\n","plt.xticks(ticks=[0, 1], labels=['Négatif', 'Positif'])\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"xz6bySggWcNn"},"source":["## Longueur des Tweets\n","Un histogramme pour explorer la longueur des tweets et voir s'il existe des différences significatives entre les tweets positifs et négatifs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":9567,"status":"ok","timestamp":1715362651838,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"ToG7PZ52WcNn","outputId":"e8489f72-fea7-4d89-b617-4dbe1086afe2"},"outputs":[],"source":["# Calcul de la longueur de chaque tweet\n","data_clean['length'] = data_clean['text'].apply(lambda x: len(x.split()))\n","\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data=data_clean, x='length', hue='target', bins=30, kde=True)\n","plt.title('Distribution de la Longueur des Tweets')\n","plt.xlabel('Longueur du Tweet')\n","plt.ylabel('Nombre de Tweets')\n","plt.legend(labels=['Négatif', 'Positif'])\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"t4BBm-bJWcNn"},"source":["## Nuages de Mots (Word Clouds)\n","Les nuages de mots pour les tweets positifs et négatifs peuvent aider à visualiser les mots les plus fréquents associés à chaque sentiment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":876},"executionInfo":{"elapsed":95484,"status":"ok","timestamp":1715362747319,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"tCyqNRtJWcNo","outputId":"ac0ce527-cff3-4a92-c124-685afda15986"},"outputs":[],"source":["# Texte pour les tweets positifs et négatifs\n","text_positif = \" \".join(tweet for tweet in data_clean[data_clean['target'] == 1]['text'])\n","text_negatif = \" \".join(tweet for tweet in data_clean[data_clean['target'] == 0]['text'])\n","\n","# Génération du nuage de mots pour les tweets positifs\n","wordcloud_pos = WordCloud(background_color='white').generate(text_positif)\n","\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud_pos, interpolation='bilinear')\n","plt.title('Nuage de Mots pour les Tweets Positifs')\n","plt.axis('off')\n","plt.show()\n","\n","# Génération du nuage de mots pour les tweets négatifs\n","wordcloud_neg = WordCloud(background_color='white').generate(text_negatif)\n","\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud_neg, interpolation='bilinear')\n","plt.title('Nuage de Mots pour les Tweets Négatifs')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11539,"status":"ok","timestamp":1715362758856,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"W_KU_fvSWcNo"},"outputs":[],"source":["# Sauvegarde du DataFrame nettoyé dans un fichier CSV\n","data_clean.to_csv('../data/data_clean.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":5020,"status":"ok","timestamp":1715362763859,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"4MXNb6BTWcNo","outputId":"84e4a4b0-49cd-48c5-b6d6-feb2b75f5db7"},"outputs":[],"source":["# Rechargement du DataFrame à partir du fichier CSV sauvegardé\n","data_reloaded = pd.read_csv('../data/data_clean.csv')\n","\n","# Vérification des premières lignes pour s'assurer que le chargement a fonctionné correctement\n","display(data_reloaded.head())"]},{"cell_type":"markdown","metadata":{"id":"S72IvapLWcNo"},"source":["## Détection de la langue\n","La détection de la langue permettra de mieux cibler le nettoyage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125996,"status":"ok","timestamp":1715362889852,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"T8r-18tZWcNo","outputId":"f1f6a834-c5d1-489d-84b4-1c8612f16b74"},"outputs":[],"source":["# Fonction pour télécharger le modèle FastText s'il n'existe pas\n","def download_fasttext_model(model_url, model_path):\n","    if not os.path.exists(model_path):\n","        print(f\"Modèle FastText non trouvé à {model_path}. Téléchargement en cours...\")\n","        response = requests.get(model_url)\n","        with open(model_path, 'wb') as model_file:\n","            model_file.write(response.content)\n","        print(\"Téléchargement terminé.\")\n","\n","# URL du modèle FastText et chemin local pour le sauvegarder\n","model_url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n","model_path = \"lid.176.bin\"\n","\n","# Téléchargez le modèle FastText si nécessaire\n","download_fasttext_model(model_url, model_path)\n","\n","# Chargement du modèle pré-entraîné FastText pour la détection de la langue\n","model = fasttext.load_model(model_path)\n","\n","def detect_language_fasttext(text):\n","    text = str(text).replace('\\n', ' ')\n","    predictions = model.predict([text], k=1)\n","    return predictions[0][0][0].replace('__label__', '')\n","\n","def detect_languages_parallel(texts, num_threads):\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        future_to_text = {executor.submit(detect_language_fasttext, text): text for text in texts}\n","        results = []\n","        for future in as_completed(future_to_text):\n","            result = future.result()\n","            results.append(result)\n","        return results\n","\n","# Application de la détection de langue en parallèle\n","num_threads = 10  # Ajustez en fonction de votre environnement\n","languages = detect_languages_parallel(data_reloaded['text'], num_threads)\n","\n","# Stockage du résultat dans une nouvelle colonne 'language'\n","data_reloaded['language'] = languages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":718},"executionInfo":{"elapsed":6160,"status":"ok","timestamp":1715362896004,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"eW55x7ShWcNo","outputId":"017e535f-6153-459e-f0a4-ca51a84d2514"},"outputs":[],"source":["plt.figure(figsize=(10, 8))\n","top_languages = data_reloaded['language'].value_counts().head(10)\n","sns.countplot(y='language', data=data_reloaded, order=top_languages.index)\n","plt.title('Top 10 des Langues dans les Tweets')\n","plt.xlabel('Nombre de Tweets')\n","plt.ylabel('Langue')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715362896298,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"ODaAwA0zWcNo"},"outputs":[],"source":["# Filtrer le DataFrame pour ne garder que les tweets en anglais\n","data_english_only = data_reloaded[data_reloaded['language'] == 'en']"]},{"cell_type":"markdown","metadata":{"id":"v56ru2_WWcNo"},"source":["## Nettoyage du texte\n","Le nettoyage du texte impliquera la suppression des éléments inutiles comme les URLs, les mentions (@), les hashtags (#), les caractères spéciaux et les chiffres. Nous allons également convertir le texte en minuscules pour uniformiser les données."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14124,"status":"ok","timestamp":1715362910850,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"uQbL0ih3WcNo"},"outputs":[],"source":["def clean_text(text):\n","    text = text.lower()  # Convertir en minuscules\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Supprimer les URLs\n","    text = re.sub(r'\\@\\w+|\\#', '', text)  # Supprimer les mentions et hashtags\n","    text = re.sub(r'\\d+', '', text)  # Supprimer les chiffres\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer la ponctuation\n","    return text\n","\n","# Application de la fonction de nettoyage à chaque tweet\n","data_english_only.loc[:, 'text'] = data_english_only.loc[:, 'text'].apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715362910850,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"0ksD6XUqWcNo","outputId":"1faf111c-6764-4394-a5fe-e9fd6cdeed3c"},"outputs":[],"source":["print(data_english_only['text'].head())"]},{"cell_type":"markdown","metadata":{"id":"QqyciS5BWcNp"},"source":["## Tokenisation\n","La tokenisation consiste à diviser le texte en mots ou tokens. Cela peut être réalisé en utilisant des bibliothèques comme NLTK ou directement avec des méthodes pandas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":165081,"status":"ok","timestamp":1715363075929,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"SxVL3MKcWcNp","outputId":"c1413341-5ca3-425a-dd23-23173a43a634"},"outputs":[],"source":["# Vérifier si 'punkt' est déjà téléchargé\n","if 'punkt' in nltk.data.path:\n","    print(\"'punkt' tokenizer model is already downloaded.\")\n","else:\n","    nltk.download('punkt')\n","    print(\"'punkt' tokenizer model has been downloaded.\")\n","\n","# Application de la tokenisation à chaque tweet\n","# Tokenisation des tweets en utilisant .loc pour éviter les avertissements SettingWithCopyWarning\n","data_english_only['tokens'] = data_english_only['text'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715363075929,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"kg7BRN5ZWcNp","outputId":"a9787365-4885-4f7d-b885-467ac80a59f0"},"outputs":[],"source":["print(data_english_only['tokens'].head())"]},{"cell_type":"markdown","metadata":{"id":"s47R8WcvWcNp"},"source":["## Suppression des stop-words\n","Les stop-words sont des mots très communs qui sont généralement ignorés dans le traitement du langage naturel car ils n'apportent pas de valeur significative pour l'analyse. Nous allons les supprimer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8097,"status":"ok","timestamp":1715363084013,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"xQNRupJuWcNp","outputId":"6b667ee4-dc1d-4d88-e496-afc45292e6a8"},"outputs":[],"source":["# Vérifier si 'stopwords' est déjà téléchargé\n","if 'stopwords' in nltk.data.path:\n","    print(\"'stopwords' tokenizer model is already downloaded.\")\n","else:\n","    nltk.download('stopwords')\n","    print(\"'stopwords' tokenizer model has been downloaded.\")\n","\n","stop_words = set(stopwords.words('english'))\n","\n","# Fonction pour supprimer les stop-words\n","def remove_stopwords(tokens):\n","    return [word for word in tokens if word not in stop_words]\n","\n","# Application de la suppression des stop-words à chaque tweet\n","data_english_only['tokens'] = data_english_only['tokens'].apply(remove_stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715363084013,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"f17xW-D_WcNp","outputId":"ed4b16f8-c3ab-4571-9676-bbaf0911f3c3"},"outputs":[],"source":["print(data_english_only['tokens'].head())"]},{"cell_type":"markdown","metadata":{"id":"b73SZ0o0WcNp"},"source":["## Lemmatisation et stemming\n","La lemmatisation permet de ramener les mots à leur forme de base. Par exemple, \"running\" devient \"run\". Cela peut aider à réduire la complexité des données textuelles.\n","Le stemming va supprimer le sufixe ou le prefixe simplement."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268202,"status":"ok","timestamp":1715363352206,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"HNoHcVPFWcNp","outputId":"32ab9da9-372b-479e-80af-117c828b2931"},"outputs":[],"source":["# Vérifier si 'wordnet' est déjà téléchargé\n","if 'wordnet' not in nltk.data.path:\n","    nltk.download('wordnet')\n","    print(\"'wordnet' tokenizer model has been downloaded.\")\n","else:\n","    print(\"'wordnet' tokenizer model is already downloaded.\")\n","\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","# Fonction pour la lemmatisation\n","def lemmatize_tokens(tokens):\n","    return [lemmatizer.lemmatize(token) for token in tokens]\n","\n","# Fonction pour le stemming\n","def stem_tokens(tokens):\n","    return [stemmer.stem(token) for token in tokens]\n","\n","# Assurez-vous que data_english_only est un DataFrame propre et non une vue d'un autre DataFrame\n","data_english_only = data_english_only.copy()\n","\n","# Application de la lemmatisation à chaque tweet\n","data_english_only.loc[:, 'text_lemmatized'] = data_english_only['tokens'].apply(lemmatize_tokens)\n","\n","# Application de la stemmatisation à chaque tweet\n","data_english_only.loc[:, 'text_stemmed'] = data_english_only['tokens'].apply(stem_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715363352206,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"AP29Z-D3WcNp","outputId":"147ec35b-71a0-4370-edb5-c077589c825b"},"outputs":[],"source":["print(data_english_only['text_lemmatized'].head())\n","print(data_english_only['text_stemmed'].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32040,"status":"ok","timestamp":1715363384236,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"6vt6DgwnWcNp"},"outputs":[],"source":["# Sauvegarde du DataFrame nettoyé dans un fichier CSV\n","data_english_only.to_csv('../data/data_english_only.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"_Tasxcp0JYvp"},"source":["### Etape du notebook aprés traitement"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76124,"status":"ok","timestamp":1715363460348,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"GJE0HwSyWcNp","outputId":"704ed186-c927-4ab6-9527-02a37c21ca4c"},"outputs":[],"source":["# Rechargement du DataFrame\n","data_english_only = pd.read_csv('../data/data_english_only.csv')\n","\n","# Conversion des chaînes de caractères représentant des listes en listes réelles\n","data_english_only['text_lemmatized'] = data_english_only['text_lemmatized'].apply(ast.literal_eval)\n","data_english_only['text_stemmed'] = data_english_only['text_stemmed'].apply(ast.literal_eval)\n","\n","# Vérification\n","print(data_english_only['text_lemmatized'].head())\n","print(data_english_only['text_stemmed'].head())"]},{"cell_type":"markdown","metadata":{"id":"-uZ__55XWcNq"},"source":["## Modélisation\n","\n","### Approche Modèle sur Mesure Simple\n","Dans cette section, nous développerons un modèle de régression logistique comme approche simple pour prédire le sentiment des tweets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715363460348,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"vJrPbeWeWcNu"},"outputs":[],"source":["#Cette fonction prend en charge l'entraînement d'un modèle donné et son évaluation sur un ensemble de test,\n","# en affichant des métriques clés comme l'accuracy, la précision, le rappel et le score F1.\n","def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n","    # Entraînement du modèle\n","    model.fit(X_train, y_train)\n","\n","    # Prédiction sur l'ensemble de test\n","    y_pred = model.predict(X_test)\n","\n","    # Affichage du rapport de classification\n","    print(classification_report(y_test, y_pred))\n","\n","    # Calcul des métriques\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='binary')\n","    recall = recall_score(y_test, y_pred, average='binary')\n","    f1 = f1_score(y_test, y_pred, average='binary')\n","\n","    # Retourner les métriques pour éventuellement les logger dans MLFlow\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","#Cette fonction simplifie la recherche d'hyperparamètres pour n'importe quel modèle, en utilisant GridSearchCV\n","def grid_search(model, param_grid, X_train, y_train, scoring='accuracy', cv=5, stratify=None):\n","    if stratify is not None:\n","        cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n","\n","    grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=cv, n_jobs=-1)\n","    grid_search.fit(X_train, y_train)\n","\n","    # Affichage des meilleurs paramètres\n","    print(\"Meilleurs paramètres : \", grid_search.best_params_)\n","\n","    return grid_search.best_estimator_\n","# Fonction pour enregistrer systématiquement les modèles et les métriques de MLFlow.\n","def log_model_metrics(model, model_name, metrics, params=None):\n","    with mlflow.start_run(run_name=model_name):\n","        # Enregistrement des paramètres du modèle, s'ils existent\n","        if params:\n","            mlflow.log_params(params)\n","\n","        # Enregistrement des métriques\n","        mlflow.log_metrics(metrics)\n","\n","        # Enregistrement du modèle\n","        mlflow.sklearn.log_model(model, model_name)\n","\n","def plot_roc_curve(model, X_test, y_test):\n","    # Prédire les probabilités pour l'ensemble de test.\n","    # Utilise decision_function ou predict_proba en fonction du modèle\n","    if hasattr(model, \"decision_function\"):\n","        y_score = model.decision_function(X_test)\n","    else:\n","        y_score = model.predict_proba(X_test)[:, 1]\n","\n","    # Calculer les taux de vrais positifs et de faux positifs\n","    fpr, tpr, _ = roc_curve(y_test, y_score)\n","\n","    # Calculer l'aire sous la courbe ROC\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Affichage de la courbe ROC\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715363460348,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"LYf6zG0OWcNu","outputId":"2bd5c936-a0d3-4f3c-b93c-32720b84805c"},"outputs":[],"source":["print(data_english_only['text_lemmatized'].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1259,"status":"ok","timestamp":1715363461603,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"_RIaD0TqWcNu"},"outputs":[],"source":["data_english_only['text_lemmatized'] = data_english_only['text_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n","if not any(data_english_only['text_lemmatized'].str.strip()):\n","    raise ValueError(\"Après la prétraitement, il ne reste aucun mot dans le texte.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":950,"status":"ok","timestamp":1715363462927,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"-sbcaSDXWcNu","outputId":"fc265e4c-4f19-4f41-97c1-dfdbd4c74632"},"outputs":[],"source":["# Calcul de la longueur des textes lemmatisés\n","data_english_only['text_length'] = data_english_only['text_lemmatized'].apply(len)\n","\n","# Affichage d'un histogramme des longueurs de texte\n","plt.figure(figsize=(10, 6))\n","plt.hist(data_english_only['text_length'], bins=30, color='skyblue')\n","plt.title('Distribution des Longueurs des Tweets après Prétraitement')\n","plt.xlabel('Longueur des Tweets')\n","plt.ylabel('Nombre de Tweets')\n","plt.grid(axis='y', alpha=0.75)\n","plt.show()\n","\n","# Vérification si certains textes sont vides\n","empty_texts = data_english_only['text_lemmatized'] == ''\n","print(f\"Nombre de textes vides après prétraitement : {empty_texts.sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"ETYunPFmWcNu"},"source":["### modelisation avec lemmatisation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":770},"executionInfo":{"elapsed":404453,"status":"ok","timestamp":1715363867372,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"bOxgxTcQWcNv","outputId":"41968def-4470-4a8e-c6aa-cb0ffa49c658"},"outputs":[],"source":["mlflow.set_experiment(\"Regression Logistique\")\n","\n","with mlflow.start_run(run_name=\"TF-IDF_Logistic_Regression_lemming\"):\n","    # Séparation des données\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        data_english_only['text_lemmatized'],\n","        data_english_only['target'],\n","        test_size=0.2,\n","        random_state=42\n","    )\n","\n","    # Création d'un pipeline avec TF-IDF et régression logistique\n","    pipeline = make_pipeline(\n","        TfidfVectorizer(max_features=1000),\n","        LogisticRegression(solver='liblinear', max_iter=1000)\n","    )\n","\n","    # Définition de la grille de paramètres pour la recherche\n","    param_grid = {\n","        'tfidfvectorizer__max_features': [1000, 5000],\n","        'logisticregression__C': [0.1, 1, 10],\n","        'logisticregression__penalty': ['l1', 'l2']\n","    }\n","\n","    # Recherche d'hyperparamètres\n","    stratify = data_english_only['target']\n","    best_model = grid_search(pipeline, param_grid, X_train, y_train, stratify=stratify)\n","\n","    # Entraînement et évaluation du meilleur modèle trouvé\n","    metrics = train_evaluate_model(best_model, X_train, y_train, X_test, y_test)\n","    \n","    # Enregistrement du modèle et des métriques\n","    mlflow.log_params(best_model.get_params())\n","    # Enregistrement des métriques\n","    mlflow.log_metrics(metrics)\n","    # Enregistrement du modèle\n","    mlflow.sklearn.log_model(best_model, 'TF-IDF Logistic Regression lemming')\n","\n","    plot_roc_curve(best_model, X_test, y_test)\n","mlflow.end_run()"]},{"cell_type":"markdown","metadata":{"id":"pS3pHciXWcNv"},"source":["### les résultats :\n","\n","- Precision pour la classe 0 (supposée être la classe négative) est de 0.79, ce qui signifie que 79 % des prédictions de la classe 0 étaient correctes.\n","- Recall pour la classe 0 est de 0.75, ce qui signifie que 75 % des instances réelles de la classe 0 ont été correctement prédites par le modèle.\n","- F1-score pour la classe 0 est de 0.77, ce qui est une moyenne harmonique entre la précision et le rappel, fournissant une mesure unique de la performance du modèle pour cette classe.\n","\n","### De même, pour la classe 1 (supposée être la classe positive) :\n","\n","- Precision est de 0.76, Recall est de 0.80, et F1-score est de 0.78.\n","\n","- L'accuracy globale du modèle est de 78 %, ce qui signifie que le modèle a correctement prédit la classe de 78 % des tweets dans l'ensemble de test.\n","- Les scores macro avg et weighted avg fournissent des moyennes des métriques respectives sur les deux classes, tenant compte de l'équilibre ou du déséquilibre entre les classes."]},{"cell_type":"markdown","metadata":{"id":"mc4rFR3jWcNv"},"source":["### modelisation avec stemmatisation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1153,"status":"ok","timestamp":1715363868512,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"DFjoURPPWcNv"},"outputs":[],"source":["# Convertir les listes de mots stemmés en chaînes de caractères\n","data_english_only['text_stemmed'] = data_english_only['text_stemmed'].apply(' '.join)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715363868513,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"da0oJlTYWcNv","outputId":"eedaca5c-a632-4652-84e0-30928a4f9a59"},"outputs":[],"source":["print(data_english_only['text_lemmatized'].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":388207,"status":"ok","timestamp":1715364256718,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"SoMMu96fWcNv","outputId":"55ba536a-6c3f-4ff1-efcc-92fb8c1691c2"},"outputs":[],"source":["mlflow.set_experiment(\"Regression Logistique\")\n","\n","with mlflow.start_run(run_name=\"TF-IDF_Logistic_Regression_stemming\"):\n","    # Séparation des données\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        data_english_only['text_stemmed'],\n","        data_english_only['target'],\n","        test_size=0.2,\n","        random_state=42\n","    )\n","\n","    # Création d'un pipeline avec TF-IDF et régression logistique\n","    pipeline = make_pipeline(\n","        TfidfVectorizer(max_features=1000),\n","        LogisticRegression(solver='liblinear', max_iter=1000)\n","    )\n","\n","    # Définition de la grille de paramètres pour la recherche\n","    param_grid = {\n","        'tfidfvectorizer__max_features': [1000, 5000],\n","        'logisticregression__C': [0.1, 1, 10],\n","        'logisticregression__penalty': ['l1', 'l2']\n","    }\n","\n","    # Recherche d'hyperparamètres\n","    stratify = data_english_only['target']\n","    best_model = grid_search(pipeline, param_grid, X_train, y_train, stratify=stratify)\n","\n","    # Entraînement et évaluation du meilleur modèle trouvé\n","    metrics = train_evaluate_model(best_model, X_train, y_train, X_test, y_test)\n","\n","\n","    # Enregistrement du modèle et des métriques\n","    mlflow.log_params(best_model.get_params())\n","    # Enregistrement des métriques\n","    mlflow.log_metrics(metrics)\n","    # Enregistrement du modèle\n","    mlflow.sklearn.log_model(best_model, 'TF-IDF Logistic Regression stemming')\n","\n","    plot_roc_curve(best_model, X_test, y_test)\n","mlflow.end_run()\n"]},{"cell_type":"markdown","metadata":{"id":"23W7-1q-WcNv"},"source":["#### Pour la stemmatisation, les métriques sont les suivantes :\n","\n","- un AUC de 0.85 pour la courbe ROC\n","- Precision pour la classe 0: 0.79\n","- Recall pour la classe 0: 0.75\n","- F1-score pour la classe 0: 0.77\n","- Precision pour la classe 1: 0.76\n","- Recall pour la classe 1: 0.79\n","- F1-score pour la classe 1: 0.78\n","- Accuracy globale: 77%\n","\n","Pour la lemmatisation, les métriques sont les suivantes :\n","\n","- un AUC de 0.85 pour la courbe ROC\n","- Precision pour la classe 0: 0.79\n","- Recall pour la classe 0: 0.75\n","- F1-score pour la classe 0: 0.77\n","- Precision pour la classe 1: 0.76\n","- Recall pour la classe 1: 0.80\n","- F1-score pour la classe 1: 0.78\n","- Accuracy globale: 78%\n","\n","#### En comparant ces métriques :\n","\n","- Accuracy: La lemmatisation donne une accuracy globalement 1% plus élevée que le stemming. Bien que la différence soit petite, cela pourrait signifier que la lemmatisation a été légèrement meilleure pour généraliser sur l'ensemble de test.\n","\n","- Precision et Recall: La lemmatisation semble avoir un meilleur rappel pour la classe 1 que le stemming (0.80 contre 0.79), ce qui signifie qu'elle était meilleure pour identifier correctement les instances réelles de la classe positive. La précision est la même pour la classe 0 dans les deux méthodes, et légèrement inférieure pour la classe 1 avec le stemming.\n","\n","- F1-Score: Pour les deux méthodes, le score F1 est identique pour la classe 0 et légèrement supérieur pour la classe 1 avec la lemmatisation. Cela indique que la lemmatisation peut être légèrement meilleure en termes d'équilibre entre la précision et le rappel pour la classe positive.\n","\n","- AUC: les deux méthodes ont des performances comparables en termes de séparation des classes.\n","\n","En conclusion, les résultats suggèrent que la lemmatisation a légèrement surpassé le stemming dans cet ensemble de données pour le modèle spécifique utilisé (régression logistique avec vectorisation TF-IDF)."]},{"cell_type":"markdown","metadata":{"id":"Sr77vB-3WcNv"},"source":["### FastText"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":430399,"status":"ok","timestamp":1715364687530,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"XxIEDTWnWcNv","outputId":"5e2f036a-59cb-474f-a555-7732d3da9068"},"outputs":[],"source":["# Fonction pour télécharger FastText model s'il n'existe pas déjà\n","def download_fasttext_model(url, output_dir, model_file_name):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    model_path = os.path.join(output_dir, model_file_name)\n","    if not os.path.isfile(model_path):\n","        response = requests.get(url)\n","        with zipfile.ZipFile(BytesIO(response.content)) as model_zip:\n","            model_zip.extractall(output_dir)\n","        print(f\"Le modèle FastText a été téléchargé et extrait dans {output_dir}.\")\n","    else:\n","        print(f\"Le modèle FastText est déjà disponible dans {output_dir}.\")\n","\n","# Fonction pour convertir les textes en vecteurs FastText moyens\n","def text_to_fasttext_vector(texts, model):\n","    vectors = []\n","    for text in texts:\n","        vector = np.mean(\n","            [model[word] for word in text if word in model.key_to_index] or\n","            [np.zeros(model.vector_size)],\n","            axis=0\n","        )\n","        vectors.append(vector)\n","    return np.array(vectors)\n","\n","def vector_for_text(text, model):\n","    word_vectors = np.array([model[word] for word in text if word in model.key_to_index])\n","    if word_vectors.size:\n","        return np.mean(word_vectors, axis=0)\n","    else:\n","        return np.zeros(model.vector_size)\n","\n","def text_to_fasttext_vector_parallel(texts, model):\n","    with ThreadPoolExecutor(max_workers=10) as executor:  # Ajustez max_workers selon vos ressources\n","        result_vectors = list(executor.map(lambda text: vector_for_text(text, model), texts))\n","    return np.array(result_vectors)\n","\n","# Téléchargez le modèle FastText si nécessaire\n","fasttext_model_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n","fasttext_output_dir = \"fasttext_model\"\n","fasttext_model_file_name = \"wiki-news-300d-1M.vec\"\n","download_fasttext_model(fasttext_model_url, fasttext_output_dir, fasttext_model_file_name)\n","\n","mlflow.set_experiment(\"Regression Logistique\")\n","\n","with mlflow.start_run(run_name=\"FastText_logistic_regression\"):\n","    # Charger le modèle FastText\n","    fasttext_model_path = os.path.join(fasttext_output_dir, fasttext_model_file_name.replace('.zip', '.vec'))\n","    fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path)\n","\n","    # Continuez avec la séparation de données, l'entraînement et l'évaluation...\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        data_english_only['text_lemmatized'].str.split().apply(lambda x: [word for word in x if word in fasttext_model.key_to_index]),\n","        data_english_only['target'],\n","        test_size=0.2,\n","        random_state=42\n","    )\n","\n","    # Convertissez les textes en vecteurs\n","    X_train_vectors = text_to_fasttext_vector_parallel(X_train, fasttext_model)\n","    X_test_vectors = text_to_fasttext_vector_parallel(X_test, fasttext_model)\n","\n","    # Créez et évaluez le modèle de régression logistique\n","    model = LogisticRegression(solver='liblinear', max_iter=1000)\n","    metrics = train_evaluate_model(model, X_train_vectors, y_train, X_test_vectors, y_test)\n","\n","    # Enregistrement du modèle et des métriques\n","    mlflow.log_params(model.get_params())\n","    # Enregistrement des métriques\n","    mlflow.log_metrics(metrics)\n","    # Enregistrement du modèle\n","    mlflow.sklearn.log_model(model, 'FastText logistic regression')\n","\n","    plot_roc_curve(model, X_test_vectors, y_test)\n","mlflow.end_run()\n"]},{"cell_type":"markdown","metadata":{"id":"CIF29QgHWcNv"},"source":["### Approche Modèle sur Mesure Avancé\n","Nous élaborerons ensuite un modèle plus complexe, utilisant des réseaux de neurones profonds, et comparerons les performances avec différents embeddings de mots."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715364687531,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"PErJo7g1stqi"},"outputs":[],"source":["# Préparation des données\n","def prepare_data(data, model_name):\n","    # Encodage des labels\n","    label_encoder = LabelEncoder()\n","    data_labels = label_encoder.fit_transform(data['target'])\n","\n","    # Division des données en ensembles d'entraînement et de test\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        data['text_lemmatized'], data_labels, test_size=0.2, random_state=42)\n","\n","    # Chemin du fichier tokenizer\n","    tokenizer_path = f'../models/{model_name}_tokenizer.pickle'\n","    if os.path.exists(tokenizer_path):\n","        # Chargement du tokenizer existant\n","        with open(tokenizer_path, 'rb') as handle:\n","            tokenizer = pickle.load(handle)\n","        print(\"Tokenizer chargé.\")\n","    else:\n","        # Création et ajustement d'un nouveau tokenizer\n","        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","        tokenizer.fit_on_texts(X_train)\n","        # Sauvegarde du nouveau tokenizer\n","        with open(tokenizer_path, 'wb') as handle:\n","            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        print(\"Nouveau tokenizer créé et sauvegardé.\")\n","\n","    # Conversion des textes en séquences d'indices\n","    X_train_seq = tokenizer.texts_to_sequences(X_train)\n","    X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","    # Padding des séquences pour qu'elles aient toutes la même longueur\n","    X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=100)\n","    X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=100)\n","\n","    return np.array(X_train_pad, dtype=np.int32), np.array(X_test_pad, dtype=np.int32), y_train, y_test, tokenizer, X_train_seq, X_test_seq, X_train_pad, X_test_pad\n","\n","\n","def build_embedding_model(input_dim, output_dim, input_length):\n","    model = Sequential([\n","        Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length, embeddings_initializer=GlorotUniform()),\n","        GlobalAveragePooling1D(),\n","        BatchNormalization(),\n","        Dense(10, activation='relu'),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def build_lstm_model(input_dim, output_dim, input_length):\n","    model = Sequential([\n","        Embedding(input_dim=input_dim, output_dim=output_dim, embeddings_initializer=GlorotUniform()),\n","        LSTM(64, kernel_regularizer=l2(0.01)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def f1_score(precision, recall):\n","    return 2 * ((precision * recall) / (precision + recall + 1e-7))\n","\n","class F1Callback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","        precision = logs.get('val_precision')\n","        recall = logs.get('val_recall')\n","        f1 = f1_score(precision, recall)\n","        logs['val_f1'] = f1\n","        print(f\" - val_f1: {f1:.4f}\")\n","        mlflow.log_metrics({\"val_f1\": f1}, step=epoch)\n","\n","def build_cnn_model(input_dim, output_dim, input_length):\n","    model = Sequential([\n","        Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length, embeddings_initializer=GlorotUniform()),\n","        Conv1D(64, 5, activation='relu'),\n","        BatchNormalization(),\n","        GlobalMaxPooling1D(),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n","    return model\n","\n","def train_and_save_model(xp_name, model_name, model, X_train, y_train, X_test, y_test, model_path, batch_size=32, epochs=10):\n","    mlflow.set_experiment(xp_name)\n","    \n","    with mlflow.start_run(run_name=model_name):\n","        mlflow.tensorflow.autolog(log_models=True)\n","        mlflow.log_params({\"batch_size\": batch_size, \"epochs\": epochs})\n","        \n","        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n","        model_checkpoint = ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_loss', mode='min')\n","        f1_callback = F1Callback()\n","        \n","        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size,\n","                            callbacks=[early_stopping, model_checkpoint, f1_callback])\n","        \n","        model.save(f'../models/{model_path}', save_format='tf')\n","        print(\"Modèle entraîné et sauvegardé.\")\n","        \n","        best_epoch = early_stopping.stopped_epoch - 3\n","        best_val_loss = history.history['val_loss'][best_epoch]\n","        best_val_accuracy = history.history['val_accuracy'][best_epoch]\n","        mlflow.log_metrics({\"best_val_loss\": best_val_loss, \"best_val_accuracy\": best_val_accuracy})\n","        \n","def plot_roc_curve_tf(y_true, y_scores):\n","    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def plot_roc_curve_BERT(y_true, model, X_test):\n","    predictions = model.predict(X_test).logits\n","    y_scores = tf.nn.softmax(predictions, axis=-1)[:, 1].numpy()\n","\n","    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"lmpm-VwUuBde"},"source":["#### Préparation des données\n","Nous commencerons par préparer les données, en les tokenisant et en les transformant en séquences numériques, puis en appliquant un padding pour assurer des longueurs uniformes."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":45982,"status":"ok","timestamp":1715364733511,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"0dOFW-Rko9_w"},"outputs":[],"source":["tf.keras.backend.clear_session()  # Efface la session actuelle\n","# Préparation des données\n","X_train, X_test, y_train, y_test, tokenizer, X_train_seq, X_test_seq, X_train_pad, X_test_pad = prepare_data(data_english_only, 'embedding_model')"]},{"cell_type":"markdown","metadata":{"id":"DZftrgeoo9aQ"},"source":["#### Modèle de base TensorFlow/Keras avec Embedding"]},{"cell_type":"markdown","metadata":{"id":"pbf8ce_CuLW2"},"source":["##### Entraînement du modèle\n","L'entraînement du modèle inclut l'utilisation d'un callback EarlyStopping pour éviter le surapprentissage."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1715364733925,"user":{"displayName":"cyrille guillaud","userId":"14844714989257214575"},"user_tz":-120},"id":"Kk3tMY8BrCt1","outputId":"1cdf531b-650a-4cc8-9efc-ac016498b487"},"outputs":[],"source":["# Paramètres de base pour les modèles\n","input_dim = len(tokenizer.word_index) + 1\n","output_dim = 50  # Dimension de sortie pour l'embedding\n","input_length = 100  # Longueur de l'input pour les modèles de réseau de neurones\n","\n","# Construction et entraînement du modèle Embedding\n","embedding_model = build_embedding_model(input_dim, output_dim, input_length)\n","train_and_save_model(\"TensorFlow/Keras avec Embedding\", \"tf_keras_embedding\",embedding_model, X_train_pad, y_train, X_test_pad, y_test, 'embedding_model.keras', batch_size=64)"]},{"cell_type":"markdown","metadata":{"id":"A6JTWNuhuOMi"},"source":["##### Tracé de la courbe ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2jR1NPZtHFE"},"outputs":[],"source":["# Prédictions et courbe ROC\n","y_pred_proba = embedding_model.predict(X_test_pad).ravel()\n","plot_roc_curve_tf(y_test, y_pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"XDYePh7upCSe"},"source":["#### Modèle TensorFlow/Keras avec Embedding et couche LSTM"]},{"cell_type":"markdown","metadata":{},"source":["#### Préparation des données\n","Nous commencerons par préparer les données, en les tokenisant et en les transformant en séquences numériques, puis en appliquant un padding pour assurer des longueurs uniformes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.keras.backend.clear_session()  # Efface la session actuelle\n","# Préparation des données\n","X_train, X_test, y_train, y_test, tokenizer, X_train_seq, X_test_seq, X_train_pad, X_test_pad = prepare_data(data_english_only, 'lstm_model')"]},{"cell_type":"markdown","metadata":{"id":"J-CK0JLsuGFu"},"source":["##### Entraînement du modèle\n","L'entraînement du modèle inclut l'utilisation d'un callback EarlyStopping pour éviter le surapprentissage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZFhmIdptdgt"},"outputs":[],"source":["# Entraînement du modèle LSTM\n","model_lstm = build_lstm_model(input_dim, 100, 100)\n","train_and_save_model(\"TensorFlow/Keras avec Embedding et couche LSTM\", \"tf_keras_embedding_LSTM\",model_lstm, X_train, y_train, X_test, y_test, 'lstm_model.keras', batch_size=64)"]},{"cell_type":"markdown","metadata":{"id":"I4RFTGRNt2g8"},"source":["##### Tracé de la courbe ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMuRSiP4tfH1"},"outputs":[],"source":["# Prédictions et courbe ROC\n","y_pred_proba = model_lstm.predict(X_test_pad).ravel()\n","plot_roc_curve_tf(y_test, y_pred_proba)"]},{"cell_type":"markdown","metadata":{"id":"6017dXSzpGpf"},"source":["#### Modèle TensorFlow/Keras avec Embedding et Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{},"source":["#### Préparation des données\n","Nous commencerons par préparer les données, en les tokenisant et en les transformant en séquences numériques, puis en appliquant un padding pour assurer des longueurs uniformes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.keras.backend.clear_session()  # Efface la session actuelle\n","# Préparation des données\n","X_train, X_test, y_train, y_test, tokenizer, X_train_seq, X_test_seq, X_train_pad, X_test_pad = prepare_data(data_english_only, 'cnn_model')"]},{"cell_type":"markdown","metadata":{"id":"R9ashujLug4F"},"source":["##### Entraînement du modèle\n","Nous utiliserons l'arrêt précoce pour éviter le surajustement pendant l'entraînement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMHvvS0HuiiH"},"outputs":[],"source":["# Paramètres de base pour les modèles\n","input_dim = len(tokenizer.word_index) + 1\n","output_dim = 50  # Dimension de sortie pour l'embedding\n","input_length = 100  # Longueur de l'input pour les modèles de réseau de neurones\n","# Entraînement du modèle CNN\n","model_cnn = build_cnn_model(input_dim, 100, 100)\n","train_and_save_model(\"TensorFlow/Keras avec Embedding et Convolutional Neural Network (CNN)\", \"tf_keras_embedding_CNN\",model_cnn, X_train, y_train, X_test, y_test, 'cnn_model.keras', batch_size=64, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"S95l7vcHunyn"},"source":["##### Tracé de la courbe ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XPU5RqBunyy"},"outputs":[],"source":["# Prédictions et courbe ROC\n","y_pred_proba = model_cnn.predict(X_test_pad).ravel()\n","plot_roc_curve_tf(y_test, y_pred_proba)"]},{"cell_type":"markdown","metadata":{},"source":["#### Préparation des données\n","Nous commencerons par préparer les données, en les tokenisant et en les transformant en séquences numériques, puis en appliquant un padding pour assurer des longueurs uniformes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.keras.backend.clear_session()  # Efface la session actuelle\n","# Préparation des données\n","X_train, X_test, y_train, y_test, tokenizer, X_train_seq, X_test_seq, X_train_pad, X_test_pad = prepare_data(data_english_only, 'bert_model')"]},{"cell_type":"markdown","metadata":{"id":"Y16Phrd_uxqE"},"source":["### BERT"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()  # Mettre le modèle en mode évaluation\n","    predictions, true_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = {'input_ids': batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels': batch[2]}\n","            outputs = model(**inputs)\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            labels = inputs['labels'].cpu().numpy()\n","\n","            predictions.extend(preds)\n","            true_labels.extend(labels)\n","\n","    precision = precision_score(true_labels, predictions, average='weighted')\n","    recall = recall_score(true_labels, predictions, average='weighted')\n","    f1 = f1_score(true_labels, predictions, average='weighted')\n","    cm = confusion_matrix(true_labels, predictions)\n","\n","    return precision, recall, f1, cm\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    Cette fonction affiche la matrice de confusion.\n","    La normalisation peut être ajoutée en fixant `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in enumerate(np.ndindex(cm.shape)):\n","        plt.text(j, i, format(cm[i, j], '.2f'),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Configuration du modèle BERT pour le fine-tuning\n","Configurez le modèle BERT pour le fine-tuning avec les données chargées :\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import ast\n","import pandas as pd\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.metrics import Precision, Recall\n","import os\n","import time\n","import mlflow\n","\n","# Définir l'expérience MLflow\n","mlflow.set_experiment('BERT')\n","# MLflow tracking\n","mlflow.tensorflow.autolog()\n","\n","with mlflow.start_run(run_name=\"BERT\"):\n","    # Chargement des données\n","    data_english_only = pd.read_csv('../data/data_english_only.csv')\n","\n","    # Conversion des chaînes de caractères représentant des listes en listes Python réelles\n","    data_english_only['text_lemmatized'] = data_english_only['text_lemmatized'].apply(ast.literal_eval)\n","\n","    # Reconstruction des textes lemmatisés en une seule chaîne de caractères pour chaque exemple\n","    texts = data_english_only['text_lemmatized'].apply(' '.join)\n","\n","    # Labels de sentiment\n","    labels = data_english_only['target'].values\n","\n","    # Initialisation du tokenizer TinyBERT\n","    tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n","\n","    # Tokenisation des textes\n","    encodings = tokenizer(texts.tolist(), truncation=True, padding='max_length', max_length=128, return_tensors='tf')\n","\n","    # Convertir les tenseurs en tableaux NumPy\n","    input_ids = encodings['input_ids'].numpy()\n","    attention_masks = encodings['attention_mask'].numpy()\n","\n","    # Division en train et test avec stratification\n","    train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, stratify=labels)\n","    train_attention_masks, val_attention_masks = train_test_split(attention_masks, test_size=0.1, stratify=labels)\n","\n","    # Convertir les tableaux NumPy en tenseurs TensorFlow\n","    train_input_ids = tf.convert_to_tensor(train_input_ids)\n","    val_input_ids = tf.convert_to_tensor(val_input_ids)\n","    train_attention_masks = tf.convert_to_tensor(train_attention_masks)\n","    val_attention_masks = tf.convert_to_tensor(val_attention_masks)\n","    train_labels = tf.convert_to_tensor(train_labels)\n","    val_labels = tf.convert_to_tensor(val_labels)\n","\n","    # Création des DataLoaders\n","    batch_size = 64\n","    train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_masks}, train_labels)).shuffle(len(train_input_ids)).batch(batch_size)\n","    val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_input_ids, 'attention_mask': val_attention_masks}, val_labels)).batch(batch_size)\n","\n","    # Initialisation du modèle TinyBERT\n","    model = TFBertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2, from_pt=True)\n","\n","    # Utiliser l'optimiseur legacy.Adam pour meilleure performance sur M1/M2\n","    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n","\n","    # Compiler le modèle avec la fonction de perte appropriée et les métriques\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', Precision(), Recall()])\n","\n","    # Vérifier si un checkpoint existe\n","    checkpoint_path = \"checkpoints\"\n","    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_path)\n","\n","    if latest_checkpoint:\n","        model.load_weights(latest_checkpoint)\n","        print(f\"Resuming from checkpoint {latest_checkpoint}\")\n","\n","    # Entraînement du modèle\n","    epochs = 3\n","    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_freq='epoch')\n","\n","    for epoch in range(epochs):\n","        history = model.fit(train_dataset, validation_data=val_dataset, epochs=1, callbacks=[checkpoint_callback])\n","\n","        # Log des métriques dans MLflow\n","        mlflow.log_metrics({\"loss\": history.history['loss'][0], \"accuracy\": history.history['accuracy'][0], \"precision\": history.history['precision'][0], \"recall\": history.history['recall'][0]}, step=epoch)\n","        mlflow.log_metrics({\"val_loss\": history.history['val_loss'][0], \"val_accuracy\": history.history['val_accuracy'][0], \"val_precision\": history.history['val_precision'][0], \"val_recall\": history.history['val_recall'][0]}, step=epoch)\n","\n","        print(f\"End of epoch {epoch+1}, pausing for 2 minutes.\")\n","        time.sleep(120)\n","\n","    output_dir = \"../models/finetuned_bert_fr/\"\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","    mlflow.end_run()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
